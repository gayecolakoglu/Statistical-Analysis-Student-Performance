---
title: "FinalProject"
author: "Gaye Çolakoğlu"
date: '2022-06-06'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
The scope of this project is the analyze students' performance in exams from the datasets generated by the Royce Kimmons(http://roycekimmons.com/tools/generated_data/exams). 

In this project, we will try to analyze and understand the influence of the parents' background, test preparation, etc on student's performance

The difference between score in none and completed test prepration groups try to clarify. The most higher score will be handled and try to figure out the main reasons for this scores is higher.

The score differences between the parental education try to be handled, and the question of ‘Is there a specific difference in scores between the parental education’ tries to be answered. 

In the last part, the project tries to find if there is an association between having lunch and math scores.


**Importing Required Libraries**
```{r}
Packages <- c( "scatterplot3d", "plot3D","plyr",  "magrittr", "data.table")
libs <- suppressWarnings(suppressMessages(lapply(Packages, library, character.only = TRUE)))

#install.packages('Rmisc')
library(Rmisc)
library(corrplot)
library(Hmisc)
library(reshape2)
library(tidyverse)
library(gridExtra)
library(dplyr)      # data manipulation
library(ggplot2)    # data visualization
library(ggpubr)     # normality test
library(naniar)     # check for missing values
```

# 1. Data

```{r}
student_df <- read.csv("C:\\Users\\Gaye\\OneDrive\\Masaüstü\\dersler\\thirdYear\\secondSemester\\statistcalComputing\\StudentsPerformance.csv")
#View(student_df)
head(student_df, 10)
```

**Explanation of the Columns**

- gender: Information about student gender
- race.ethnicity: Information about race/ethnicity
- parental.level.of.education: Information about parental level of education
- lunch: Information about student's lunch
- test.preparation.course: Information about student's test preparation course
- math.score: Information about student's math score
- reading.score: Information about student's reading score
- writing.score: Information about student's writing score

Let’s run str() to get information about types of features.
```{r}
str(student_df)
```
From the above result we can see that our data contains both categorical and quantitative variables.

Let's look in a more visualize way
```{r}
library(visdat)
vis_dat(student_df)

```

gender, race.ethinicity, parental.level.of.education, lunch, test.preparation.course columns have character data and math.score, reading.score and writing.score columns have integer data.

# 2. Exploratory and Descriptive Data Analaysis

**Displaying statistical summary of the data**
```{r}
summary(student_df)
```
**Analysis of the output**

*Mean*

- The average math.score is 66.09.
- The average reading.score is 69.17.
- The average writing.score is 68.05.
- We can also assume that students are more successful in reading than in math or writing.

*Median*

- Most of the observations are the same as mean value observation.
- The difference in mean and median value is due to outliers. We can also observe this difference in categorical variables.

**Checking For Missing Values**
```{r}
miss_var_summary(student_df)
```

NA with more visualize way
```{r}
gg_miss_var(student_df)
```

**Finding Uniques in each column**
```{r}
for (i in seq(1,ncol(student_df)-3,1)){
  print(unique(student_df[i]))
}
```

**Data preparation**

Renaming the columns for the ease of use
```{r}
colnames(student_df)[2] <- "race_ethnicity"
colnames(student_df)[3] <- "parental_education"
colnames(student_df)[5] <- "test_prep"
colnames(student_df)[6] <- "math_score"
colnames(student_df)[7] <- "reading_score"
colnames(student_df)[8] <- "writing_score"
str(student_df)
```

# 3. Data Visualization

First of all, let’s look at the student population.
```{r}
# convert df to data.table 
dff <- as.data.table(student_df)

piePlotter <- function(data,y,fill, title){
  pie <- ggplot(data, aes(x="", y=y, fill= fill)) + geom_bar(stat="identity", width=2) + coord_polar("y", start=0) +  geom_text(aes(label = paste0(round(y/10), "%")), position =  position_stack(vjust = 0.5)) + labs(x = NULL, y = NULL, fill = NULL, title = title) + theme_classic() + theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), plot.title = element_text(hjust = 0.5, color = "#000000"));
}

gernderSplit<-dff[,.N,by=gender]
colnames(gernderSplit)<-c("Gender","Count")

parentEduSplit<-dff[,.N,by=parental_education]
colnames(parentEduSplit)<-c("EducationLevel","Count")
parentEduSplit <- parentEduSplit[order(Count),]

testSplit<-dff[,.N,by=test_prep]
colnames(testSplit)<-c("Test","Count")

lunchSplit<-dff[,.N,by=lunch]
colnames(lunchSplit)<-c("Lunch","Count")

byGnderPlot<-piePlotter(gernderSplit, gernderSplit$Count,gernderSplit$Gender, "Students by Gender")
byParentEduLevel<-piePlotter(parentEduSplit, parentEduSplit$Count,parentEduSplit$EducationLevel, "Students by Parent's Education Level")
byTest<-piePlotter(testSplit, testSplit$Count,testSplit$Test, "Students by Test Prep")
byLunch<-piePlotter(lunchSplit, lunchSplit$Count,lunchSplit$Lunch, "Students by type of the Lunch")


grid.arrange(byGnderPlot, byParentEduLevel,byTest, byLunch, ncol=2)

```

Let's use a more fun plot for race
```{r}
pie <- ggplot(student_df,aes(x=race_ethnicity, fill = race_ethnicity))+
  geom_bar()+
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust=0.1))

pie + coord_polar(theta = "x",start=0)
```

What we find:

- There are more female students in the population than male students
- Majority of parents of the students have studied up to college level. 40% of parents have a degree.
- Majority of the students belong to the race group C. Second largest student community is from race group D. Group A is the minor race group of the student population.
- Approximately 2/3 of the students take standard lunch.

Let’s try to dig more into students background using parent education level, lunch category and race.

**Scores By gender:**
```{r}
Data_gather <- student_df %>%
  mutate(StudentID = row_number()) %>%
  gather(key = "subject", value = "score", math_score:writing_score)

plot6 <- ggplot(data=Data_gather, aes(x=Data_gather$gender, y=Data_gather$score, 
                             fill=Data_gather$gender)) + 
  geom_boxplot() +
  stat_summary(fun.y=mean, colour="darkred", geom="point", 
               shape=18, size=3,show_guide = FALSE) +
  facet_grid(. ~ subject ) +
  theme_bw() +
  theme_classic() +
  theme(legend.title = element_blank()) +
  xlab("Gender") +
  ylab("Score") +
  scale_fill_brewer(type = "qual", palette = 1, direction = 1,
  aesthetics = "fill")

plot6
```

*AVERAGE SCORE*

- Averagely speaking, female students are doing a better job in reading and writing, while male students have a higher score in math. The difference is the biggest with writing subject.

*STANDARD DEVIATION*

- In all three subject, female students have scores that are distributed more loosely.

*SCORE DISTRIBUTION*

- Female students: For math subject, the score range 60-69 has the most number of students. For reading and writing, the score range 70-79 has the most number of students.

- Male students: For math subject, the distribution of scores is slightly right shifted compared to females' score distribution. But the most number of male students still have scores located in range 60-69. For reading subject, the distribution of scores is left shifted compared to females' score distribution. For writing subject, the shift is even more obvious. But for both subjects, the most number of male students have scores located in range 60-69.

**Scores by race/ethnicity: Average scores are highest in the group E.**
```{r}
plot5 <- ggplot(data=Data_gather, aes(x=Data_gather$race_ethnicity, y=Data_gather$score, 
                             fill=Data_gather$race_ethnicity)) + 
  geom_boxplot() + 
    theme(
    text = element_text(family = "Tahoma"),
    axis.text.x = element_blank(),
    legend.title = element_blank()
    ) +

  stat_summary(fun.y=mean, colour="darkred", geom="point", 
               shape=18, size=3,show_guide = FALSE) +
  facet_grid(. ~ subject ) +
  theme_bw() +
  theme_classic() +
  scale_fill_brewer(type = "qual", palette = 1, direction = 1,
  aesthetics = "fill") +
  xlab("Race/ethnicity") +
  ylab("Score")+
  theme(axis.text.x= element_text(angle =45, hjust=1))

plot5
```

Result
If we can say that the the people who scores higher than 80 will be more effective in society. Than it could be said that, group E is the leading one and group A is the last one. However group C seems to have highest population at all. However, if we take the proportion of passing population and general population, we can see that group E is the most successful ethnic group.

**Scores by Lunch**
```{r}
plot5 <- ggplot(data=Data_gather, aes(x=Data_gather$lunch, y=Data_gather$score, 
                             fill=Data_gather$lunch)) + 
  geom_boxplot() + 
    theme(
    text = element_text(family = "Tahoma"),
    axis.text.x = element_blank(),
    legend.title = element_blank()
    ) +

  stat_summary(fun.y=mean, colour="darkred", geom="point", 
               shape=18, size=3,show_guide = FALSE) +
  facet_grid(. ~ subject ) +
  theme_bw() +
  theme_classic() +
  scale_fill_brewer(type = "qual", palette = 1, direction = 1,
  aesthetics = "fill") +
  xlab("Race/ethnicity") +
  ylab("Score")

plot5
```

Result
As shown in the table, students with standard lunch have better performance in the tests.

**Scores by Parental Education**
```{r}
plot5 <- ggplot(data=Data_gather, aes(x=Data_gather$parental_education, y=Data_gather$score, 
                             fill=Data_gather$parental_education)) + 
  geom_boxplot() + 
    theme(
    text = element_text(family = "Tahoma"),
    axis.text.x = element_blank(),
    legend.title = element_blank()
    ) +

  stat_summary(fun.y=mean, colour="darkred", geom="point", 
               shape=18, size=3,show_guide = FALSE) +
  facet_grid(. ~ subject ) +
  theme_bw() +
  theme_classic() +
  scale_fill_brewer(type = "qual", palette = 1, direction = 1,
  aesthetics = "fill") +
  xlab("Race/ethnicity") +
  ylab("Score")+
  theme(axis.text.x= element_text(angle =45, hjust=1))

plot5
```

Result
When parents' education level goes higher, students' average score has a clear trend of increasing for all three subjects. The scores' standard deviation also goes down in general with higher parents' education level.


**Let's look into parental education effect on students success more deeply**
```{r}
#Math score
plot_df <- data.frame(parental_education = c("some_highschool","highschool","some_college","associate_degree","bachelor_degree","master_degree"),
               math = c(63.50,62.14,67.13,67.88,69.39,69.75),
               reading = c(66.94,64.70,69.46,70.93,73.00,75.37),
               writing  = c(64.89,62.45,68.84,69.90,73.38,75.68))


p1 <- ggplot(plot_df,aes(x=factor(parental_education,level=c("some_highschool","highschool","some_college","associate_degree","bachelor_degree","master_degree")),y=math,group=1))+
  geom_line(color="blue")+
  geom_label(aes(label = math, size = NULL,color=NULL), nudge_y = 0.7)+
  ggtitle("Math score")+
  xlab("Parental education")+
  ylab("Score")+ 
  theme(axis.text.x= element_text(angle =45, hjust=1))

#Reading score
p2 <- ggplot(plot_df,aes(x=factor(parental_education,level=c("some_highschool","highschool","some_college","associate_degree","bachelor_degree","master_degree")),y=reading,group=1))+
  geom_line(color="red")+
  geom_label(aes(label = reading, size = NULL,color=NULL), nudge_y = 0.7)+
  ggtitle("Reading score")+
  xlab("Parental education")+
  ylab("Score")+ 
  theme(axis.text.x= element_text(angle =45, hjust=1))

#writing score
p3 <- ggplot(plot_df,aes(x=factor(parental_education,level=c("some_highschool","highschool","some_college","associate_degree","bachelor_degree","master_degree")),y=writing,group=1))+
  geom_line(color="green")+
  geom_label(aes(label = writing, size = NULL,color=NULL), nudge_y = 0.7)+
  ggtitle("Writing score")+
  xlab("Parental education")+
  ylab("Score")+ 
  theme(axis.text.x= element_text(angle =45, hjust=1))

Rmisc::multiplot(p1,p2,p3, cols= 3)
```

Result
Generally, the results are the same. Therefore, the scores does not changes with educational level. This scores are based mostly students's tendency to educational area. However, if we look at highly taken scores because of the education system which is based on success, we see that master degree as expected is highly effective on students. After that, as expected, bachelor's degree is similarly effective on success. Explicitly, the successfull students rate to general population in system is higher at master's degree and bachler's degree level.

**Scores by Test Preparation**
```{r}
library(plyr)
mu <- ddply(student_df, "test_prep", summarise, grp.mean=mean(math_score))

# Use semi-transparent fill and Add mean lines
p<-ggplot(student_df, aes(x=math_score, fill=test_prep)) +
  geom_density(alpha=0.4)+
  geom_vline(data=mu, aes(xintercept=grp.mean, color=test_prep),
             linetype="dashed")

mu <- ddply(student_df, "test_prep", summarise, grp.mean=mean(reading_score))

# Use semi-transparent fill and Add mean lines
p1<-ggplot(student_df, aes(x=reading_score, fill=test_prep)) +
  geom_density(alpha=0.4)+
  geom_vline(data=mu, aes(xintercept=grp.mean, color=test_prep),
             linetype="dashed")

mu <- ddply(student_df, "test_prep", summarise, grp.mean=mean(writing_score))

# Use semi-transparent fill and Add mean lines
p2<-ggplot(student_df, aes(x=writing_score, fill=test_prep)) +
  geom_density(alpha=0.4)+
  geom_vline(data=mu, aes(xintercept=grp.mean, color=test_prep),
             linetype="dashed")

Rmisc::multiplot(p,p1,p2, cols= 1)

```

It is clearly seen that less than half students chose and completed preparation course. But students who completed the course behave better in the tests of all three subjects, which implies the course is effective.

```{r}
plot5 <- ggplot(data=Data_gather, aes(x=Data_gather$test_prep, y=Data_gather$score, 
                             fill=Data_gather$test_prep)) + 
  geom_boxplot() + 
    theme(
    text = element_text(family = "Tahoma"),
    axis.text.x = element_blank(),
    legend.title = element_blank()
    ) +

  stat_summary(fun.y=mean, colour="darkred", geom="point", 
               shape=18, size=3,show_guide = FALSE) +
  facet_grid(. ~ subject ) +
  theme_bw() +
  theme_classic() +
  scale_fill_brewer(type = "qual", palette = 1, direction = 1,
  aesthetics = "fill") +
  xlab("Test Prepration") +
  ylab("Score")

plot5
```

Result
Students who have completed test pre-course get higher scores on average in every subject, and they have scores with smaller standard deviation too.

The most obvious influence should be whether or not the student completed the test prep course.

# 4. Central Limit Theorem

- The mean of the sampling distribution:x 
- The mean of the population distribution:μ
- x = μ
- The standard deviation of the sampling distribution:s 
- The standard deviation of the population distribution:σ
- The sample size:n
- s = σ / n

```{r}
score <- Data_gather$score
#Calculate the population mean
mean(Data_gather$score)

#Plot all the observations in the data
hist(Data_gather$score,col = "pink",main = "Scores",xlab = "score")
abline(v=12.8,col="red",lty=1)
```

Red vertical line above is the population mean. We can also see from the above plot that the population is not normal. Therefore, we need to draw sufficient samples of different sizes and compute their means (known as sample means). We will then plot those sample means to get a normal distribution.

In our example, we will draw sufficient samples of size 10, calculate their means, and plot them in R. I know that the minimum sample size taken should be 30 but let’s just see what happens when we draw 10:

```{r}
#We will take sample size=10, samples=9000
#Calculate the arithmetice mean and plot the mean of sample 9000 times

s10<-c()
n=9000
for (i in 1:n) {
s10[i] = mean(sample(Data_gather$score,10, replace = TRUE))}
hist(s10, col ="lightgreen", main="Sample size =10",xlab = "score")
abline(v = mean(s10), col = "Red")
abline(v = 12.8, col = "blue")
```

Now, we know that we’ll get a very nice bell-shaped curve as the sample sizes increase. Let us now increase our sample size and see what we get:
```{r}
#We will take sample size=30, 50 & 500 samples=9000
#Calculate the arithmetice mean and plot the mean of sample 9000 times

s30 <- c()
s50 <- c()
s500 <- c()
n =9000
for ( i in 1:n){
s30[i] = mean(sample(Data_gather$score,30, replace = TRUE))
s50[i] = mean(sample(Data_gather$score,50, replace = TRUE))
s500[i] = mean(sample(Data_gather$score,500, replace = TRUE))
}
par(mfrow=c(1,3))
hist(s30, col ="lightblue",main="Sample size=30",xlab ="wall thickness")
abline(v = mean(s30), col = "red")

hist(s50, col ="lightgreen", main="Sample size=50",xlab ="wall thickness")
abline(v = mean(s50), col = "red")

hist(s500, col ="orange",main="Sample size=500",xlab ="wall thickness")
abline(v = mean(s500), col = "red")
```

Here, we get a good bell-shaped curve and the sampling distribution approaches normal distribution as the sample sizes increase. Therefore, we can consider the sampling distributions as normal.

# 5. Confidence Intervals

Let's see the boxplot of scores and test_prep again
```{r}
plot5 <- ggplot(data=Data_gather, aes(x=Data_gather$test_prep, y=Data_gather$score, 
                             fill=Data_gather$test_prep)) + 
  geom_boxplot() + 
    theme(
    text = element_text(family = "Tahoma"),
    axis.text.x = element_blank(),
    legend.title = element_blank()
    ) +

  stat_summary(fun.y=mean, colour="darkred", geom="point", 
               shape=18, size=3,show_guide = FALSE) +
  facet_grid(. ~ subject ) +
  theme_bw() +
  theme_classic() +
  scale_fill_brewer(type = "qual", palette = 1, direction = 1,
  aesthetics = "fill") +
  xlab("Test Prepration") +
  ylab("Score")

plot5
```

This plot suggests that completing the test_prep is associated with higher score.

How can we assess whether this difference is statistically significant?

Let’s compute a summary table
```{r}
aggregate(Data_gather$score ~ Data_gather$test_prep, 
          data = Data_gather, 
          FUN = function(x) {c(mean = mean(x), sd = sd(x))})
```

```{r}
mean(Data_gather$score)
sd(Data_gather$score)
```
We assume that we don’t have an information about the population varience and we’ll use the t-score for calculating Confidence Intervals

Let’s take the none and completed one by one and compare their Confidence Intervals
```{r}
#subset the none and completed
none.vs.completed = subset(Data_gather, subset = Data_gather$test_prep == "none" | Data_gather$test_prep == "completed" )
summary(none.vs.completed)
```

```{r}
none = subset(none.vs.completed, subset = none.vs.completed$test_prep == "none")
completed = subset(none.vs.completed, subset = none.vs.completed$test_prep == "completed")
```

```{r}
#Confidence Interval of none
none.CI <- t.test(none$score)$conf.int
none.CI
#Confidence Interval of completed 
completed <- t.test(completed$score)$conf.int
completed

```
We took the 0.95 Confidence Intervals of score in none and completed. And we can see that the confidence intervals are not overlapping and scores in none and completed are significantly different from each others.

# 6. LOG TRANSFORMATION

**Check for Normality**

Let’s see how scores changes with the completed test_prep.
```{r}
all_complete <-  subset(Data_gather, subset = Data_gather$test_prep == "completed" )
head(all_complete)
```
First summarize the reading scores
```{r}
summary(all_complete$score)
```
It seems that mean is greater than 70. Now let’s look at the shape of data.
```{r}
p1 <- ggplot(all_complete, aes(sample = score)) + ylab("Math Score") +
  stat_qq() +
  stat_qq_line()

p2 <- ggplot(all_complete, aes(y = score)) +
  xlab("Math Score")
p2 <- p2 + geom_boxplot(fill=I("purple"))

p3 <- ggplot(all_complete, aes(x = score)) +
  xlab("Math Score")

p3 <- p3 + geom_density(fill=I("purple"))

ggarrange(p1, p2, p3, ncol =3, nrow = 1)
```
```{r}
shapiro.test(all_complete$score)
```
As we can see from plots and also in Shapiro(p<0.05) we can say that our data is not normally distributed, it seems left-sqewed However, let's try log transformation to see what is going to happen.We’ll have log transformation to get more normally distributed data.

```{r}
all_complete.transform <- transform(all_complete,
 score = log10(score))

base.plot <- ggplot(all_complete.transform, aes(x =
score)) +
 xlab("score")
p1 <- base.plot + geom_histogram(fill="#ff4d00")
p2 <-ggplot(all_complete.transform, aes(y = score)) +
geom_boxplot(fill="#ff4d00")
p3 <- base.plot + geom_density(fill="#ff4d00")
ggarrange(p1, p2, p3, ncol =2, nrow = 2, labels = c( "Geom
Histogram","Boxplot","Density"))
```
```{r}
shapiro.test(all_complete.transform$score)
```
As we can see from plots and also in Shapiro(p<0.05) then the null hypothesis that the data are normally distributed is rejected. However, it seems that log transformation didn’t work for transform the data to more normally distributed.

From now on we’ll assume that our data is normally distributed

# 7. Single t-test

Now let’s test our hypothesis using Welch Two Sample t-test

**a. Aim :** 
Our aim is to discover if the average reading score of students whose parental education is a bachelor's degree is higher than 70.

**b. Hypothesis and level of significance :**

- *H0:* Mu reading.score.bachelor's.parent >= 70
- *H1:* Mu reading.score.bachelor's.parent < 70
- *significant level:* 0.05. 

**c. Assumption Check :** 
We’ll assume that we don’t have an information about sample standard deviation and assume that there is no significant difference between variances, and we’ll use t-test.

**d. Indicate “which test you choose” “for what reason” :** 
I choose Welch's t-test also known as unequal variances t-test which is used when you want to test whether the means of two population are equal. This test is generally applied when the there is a difference between the variations of two populations and also when their sample sizes are unequal.

```{r}
parental.edu.bachelor <- subset(student_df, subset = student_df$parental_education == "bachelor's degree" )
head(parental.edu.bachelor)
```
First summarize the reading scores
```{r}
summary(parental.edu.bachelor$reading_score)
```
It seems that mean is greater than 70. Now let’s look at the shape of data.
```{r}
p1 <- ggplot(parental.edu.bachelor, aes(sample = reading_score)) + ylab("Reading Score") +
  stat_qq() +
  stat_qq_line()

p2 <- ggplot(parental.edu.bachelor, aes(y = reading_score)) +
  xlab("Reading Score")
p2 <- p2 + geom_boxplot(fill=I("orange"))

p3 <- ggplot(parental.edu.bachelor, aes(x = reading_score)) +
  xlab("Reading Score")

p3 <- p3 + geom_density(fill=I("orange"))

ggarrange(p1, p2, p3, ncol =3, nrow = 1)
```

```{r}
shapiro.test(parental.edu.bachelor$reading_score)
```
As we can see from plots and also in Shapiro(p>0.05) we can not reject that our data is normally distributed and we can assume that data is normally distributed. 

Now let's compute the One Sample t-test.

- *H0:* Mu reading.score.bachelor's.parent >= 70
- *H1:* Mu reading.score.bachelor's.parent < 70

```{r}
bachelor.parent.reading.test<- t.test(parental.edu.bachelor$reading_score, alternative = c("less"), mu = 70, conf.level = 0.95)
bachelor.parent.reading.test
```

**e. Result :** 
The p-value of the t-Test is: 0.98 which greater than 0.05. Due to its greater than our significance level(0.05) there is not enough evidence to reject the null hypothesis."

**f. Conclusion :** 
After computing the t-test we found the p-value > 0.05 which means that the average reading score of students whose parents have a bachelor's degree is not less than 70.

**g. What can be Type-1 and Type-2 error here :**
Type1 Error: Mu reading.score.bachelor's.parent >= 70 but we rejected the null hypothesis.

Type2 Error: Mu reading.score.bachelor's.parent < 70 but we fail to reject the null hypothesis.

# 8. PAIRED T-TEST

**a. Aim :** 
Our aim is to find out if mean of student's math score that in the test_prep completed group is equal to the mean of student's math score that in the test_prep none group.

**b. Hypothesis and level of significance :**

- *Ho:* Mu none = Mu completed 
- *H1:* Mu none != Mu summer
- *significant level:*  0.05. 

**c. Assumption Check :** 
The data is collected from a representative, randomly selected portion of the total population.
Data is normally distributed.
Sample size is large enough.
Data is homogeneous

```{r}
# Check for outliers
qplot(x = test_prep, y = score,
 geom = "boxplot", data = none.vs.completed,
 xlab = "Test_prep",
 ylab = "Math score",
 fill = I("lightgreen"))
```

The boxplot suggest that none group is associated with less math score. There are lots of outliers because of our data is skewed.

Before going for a t-test let’s check if the variance of two groups are equal with F-test.
```{r}
res.ftest <- var.test(score ~ test_prep, data =
none.vs.completed)
res.ftest

```
p-value of F-test smaller than alpha(0.05) that mean variances of two sets of data are different. But we’ll assume that the variences are equal.

**After normality check let's start t-test**
```{r}
none.completed.t.test <- with(none.vs.completed,
t.test(x=score[test_prep=="none"],
 y=score[test_prep=="completed"]))


none.completed.t.test
```
```{r}
# Calculate difference in means between completed and none groups
none.completed.diff <- round(none.completed.t.test$estimate[1] -
none.completed.t.test$estimate[2], 1)
# Confidence level as a %
conf.level <- attr(none.completed.t.test$conf.int, "conf.level") * 100

```

**d. Result :** 
Our p-value is less than 0.05(alpha) and also our confidence interval doesn't include 0. That's mean math score in none and completed group significantly different from each others. And we can reject the null hypothesis.

**e. Conclusion :** 
Since we have rejected null hypothesis, we can say that average scores of student's that in none and completed groups are not equal.

# 9. FISHER'S EXACT TEST

**Aim:** 
In this section, I would like to discover if there is an association between having lunch and math scores.

**Hyphotesis:** 

- *H0:* Having lunch and being succesfull in math score are independent.
- *H1:* Having lunch and being succesfull in math score are dependent.
- *significant level:* 0.05. 

For this test, we will consider the student who has higher than mean score as successful and otherwise not.
```{r}
mean.math.score <- mean(student_df$math_score)
mean.math.score
student.df.fisher <- student_df
student.df.fisher$math_score<-ifelse(student.df.fisher$math_score >=mean.math.score, "success", "failure")
head(student.df.fisher)
```
Let's plot the bar plot of having lunch(standard) and not(free/reduced) and corresponding math scores and try to spot any correlation. 
```{r}
ggplot(student.df.fisher, aes(lunch, ..count..)) + geom_bar(aes(fill = math_score), position = "dodge")
```

It seems that students who has lunch before school are more succesfull in math. But let's check it if these two feature are dependent or not with Fisher's test.
```{r}
fisherTest <- fisher.test(table(student.df.fisher$lunch, student.df.fisher$math_score), conf.int = 0.95)
fisherTest
```

```{r}
table <- table(student.df.fisher$lunch,
               student.df.fisher$math_score)
table
```

**c. Result:** 
The p-value of the Fisher Test is: 2.2e-16. Due to its less than our significance level(0.05) we reject the null hypothesis.

**d. Conclusion:**
According to Fisher's Exact Test, having lunch and being successful in math are dependent. This means if a student has lunch properly he/she is successful in math.

**e. Odds Ratio:** 
The odds ratio shows how many times more positive cases occur than negative cases.

The positive cases in our example are having free/reduced lunch — failure and being having standard lunch — success. 

So, this result shows, odds of failure while the student have free/reduced lunch is 3.251817 times more than failure while the student have standard lunch.

# 10. ANOVA and TUKEY TEST

**a. Aim :** 
Our aim to compare the most often three ethnicity groups of students' reading scores with prepareation completed and see if the average reading scores of each are equal or not.

**b. Hypothesis and level of significance :**

- *H0:* Mu GroupB.reading.score = Mu GroupC.reading.score = Mu GroupD.reading.score
- *H1:* At least one differs
- *significant level:*  0.05. 

**c. Assumption Check :** 
interval data of the dependent variable
normality
homoscedasticity
no multicollinearity.


*ANOVA*

List the unique ethnicities and their count.
```{r}
table((student_df$race_ethnicity))
```
It seems that most often ethnicities are *Group B*, *Group C* and *Group D*. Let's just take these three groups of ethnicty and their reading score with preperation.

```{r}
reading.groupB= subset(student_df, subset = race_ethnicity == "group B" & test_prep == "completed")
reading.groupC = subset(student_df, subset = race_ethnicity == "group C" & test_prep == "completed")
reading.groupD = subset(student_df, subset = race_ethnicity == "group D" & test_prep == "completed")

head(reading.groupB)
```
Now visualize them and see if the data normally distributed or not.

```{r}
p1 <- ggplot(reading.groupB, aes(sample = reading_score)) + ylab("GroupB Reading Score") +
  stat_qq() +
  stat_qq_line()

p2 <- ggplot(reading.groupC, aes(sample = reading_score)) + ylab("Groupc Reading Score") +
  stat_qq() +
  stat_qq_line()

p3 <- ggplot(reading.groupD, aes(sample = reading_score)) + ylab("GroupD Reading Score") +
  stat_qq() +
  stat_qq_line()

ggarrange(p1, p2, p3, ncol =3, nrow = 1, labels = c( "Before Transform","Before Transform",
                                                 "Before Transform"))
```

It seems that data are normally distiributed but let's take a Shapiro Test of each one.
```{r}
shapiro.test(reading.groupB$reading_score)
```

```{r}
shapiro.test(reading.groupC$reading_score)
```

```{r}
shapiro.test(reading.groupD$reading_score)
```
**Note:** Group B and Group D has p-value bigger than 0.05 and that means these two groups' score are normally distributed but Group C is not. For the following tests we'll assume that also the reading score of Group C is normally distributed.

Due to we assume that the data is normally distributed now we can compute the ANOVA test.

- *H0:* Mu GroupB.reading.score = Mu GroupC.reading.score = Mu GroupD.reading.score
- *H1:* At least one differs
```{r}
#Prepare the data for ANOVA
group.BCD.reading.score <- subset(student_df, subset = (race_ethnicity == "group B" | race_ethnicity == "group C" | race_ethnicity == "group D") & test_prep == "completed")

head(group.BCD.reading.score)
```

```{r}
# Compute the analysis of variance
res.aov <- aov(reading_score ~ race_ethnicity, data = group.BCD.reading.score)
# Summary of the analysis
summary(res.aov)
```
**d. Result of ANOVA:**
The p-value obtained from ANOVA is 0.839 and bigger than alpha(0.05): The differences between the means are not statistically significant. There is not enough evidence to reject the null hypothesis.

**e. Conclusion of ANOVA:** 
There is not enough evidence to reject that the average reading scores of three ethnicity groups are the same. In other words, there is strong evidence that average reading scores of three ethnicity groups are the same.


*TUKEY*

This test tells us which pairs of groups are significantly different. We find that there is not significantly difference in our pair of groups. But let’s compute the Tukey Test and see it better.
```{r}
TukeyHSD(res.aov)
```

**f. Result of Tukey:**
When we check the lower and upper bounds of all, each two group comparison all the boundaries contain 0. Therefore, there is strong evidence that these three ethnic groups' reading scores do not differ.

**g. Conclusion of Tukey:** 
There is strong evidence that these three ethnic groups' reading scores do not differ.In other words, average reading scores of three ethnicity groups are the same.

# 11. Multiple Linear Regression

**a. Aim :** 
Building a model for estimating score based on the parental_education, test_prep, gender, race_ethnicity and lunch groups.

**b. Regression Equation :**
score = b0 + b1xparental_education + b2xtest_prep + b3xlunch + b4xgender + b5*race_ethnicity

**c. Hypothesis and level of significance :** 

- *H0:* β1 = β2 = ... = βk = 0(there is no (linear) relationship between the three variables)
- *H1:* βj ≠ 0 for at least one j(there is a (linear) relationship between the three variables)
- *significant level:* 0.05. 

**d.  Find the Best Model :** 
```{r}
model <- lm(score ~ parental_education + test_prep + lunch + gender + race_ethnicity, data = Data_gather)
summary(model)
```

The t-statistic evaluates whether or not there is significant association between the predictor and the outcome variable, that is whether the beta coefficient of the predictor is significantly different from zero.

We found that parental_education, test_prep and gender are not significant in the multiple regression model. This means that, for a fixed amount of lunch and gender, changes in the gender will not significantly affect scores.

So we can remove them from the model
```{r}
model <- lm(score ~  lunch +  race_ethnicity, data = Data_gather)
summary(model)
```

Finally, our model equation can be written as follow: 
*score = 58 + 8.3xlunchstandard + 2.1xrace_ethnicitygroup B + 3.7xrace_ethnicitygroup C + 5.8xrace_ethnicitygroup D + 8.8xrace_ethnicitygroup E*

The confidence interval of the model coefficient can be extracted as follow:
```{r}
confint(model)
```

The overall quality of the model can be assessed by examining the R-squared (R2) and Residual Standard Error (RSE).

In our example, with lunch and race_ethnicity predictor variables, the adjusted R2 = 0.12, meaning that “12% of the variance in the score can be predicted by lunch and race_ethnicity.

**Residual Standard Error (RSE), or sigma**

The RSE estimate gives a measure of error of prediction. The lower the RSE, the more accurate the model (on the data in hand).

The error rate can be estimated by dividing the RSE by the mean outcome variable:
```{r}
sigma(model)/mean(Data_gather$score)
```
In our multiple regression example, the RSE is 14.26 corresponding to 21% error rate.

**e. Assumption Check :**
Linear relationship
Multivariate normality
No auto-correlation
No or little multicollinearity
Homoscedasticity

**f. Result :** 
Our study finds that our best model's RSE is 14.26 which corresponds 21% error rate and R2 is 0.1026

**g. Conclusion :**
Our p-value is less than 0.05(alpha), we conclude that there is a useful linear relationship
between y and at least one of the four predictors in the
model, so we reject the null hypothesis.

**h. Prediction :**
```{r}
#define new student
new <- data.frame(lunch=c("standard"), race_ethnicity=c("group A"))

#use the fitted model to predict the score for the new student
predict(model, newdata=new)
```
The model predicts that this new student will have a score of 83.39607.

